
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I am currently a third-year M.Sc. student at University of Chinese Academy of Sciences (UCAS), and doing my research at Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences. My research interest mainly includes trustworthy AI, machine learning, especially domain generalization, and reinforcement learning. Currently, my main focus is on investigating the out-of-distribution (ood) detection. The goal of my work is to enable fine-grained ood detection. I think the key to this work is finding a suitable compression structure to represent the in-distribution data.\n","date":1687374840,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1687910400,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"I am currently a third-year M.Sc. student at University of Chinese Academy of Sciences (UCAS), and doing my research at Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences. My research interest mainly includes trustworthy AI, machine learning, especially domain generalization, and reinforcement learning.","tags":null,"title":"Mingrong Gong","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Wowchemy’s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://gmr523.github.io/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Wowchemy's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":["Mingrong Gong"],"categories":["notes"],"content":" Brief Description Machine learning explainability and interpretability are both concepts that aim to shed light on how a machine learning model makes decisions, but they approach the goal from slightly different perspectives. Here’s a breakdown of the differences between the two:\nInterpretability pertains to the degree to which causal relationships can be discerned within a given system. Stated differently, it encapsulates the extent to which one can prognosticate outcomes in response to alterations in input variables or algorithmic parameters. It entails the capability to inspect an algorithm and readily comprehend its operational dynamics.\nExplainability denotes the magnitude to which the internal mechanics of a machine or deep learning system can be expounded using comprehensible human terminology. While the subtle divergence from interpretability might evade immediate notice, it is prudent to regard it in this manner: interpretability facilitates the discernment of mechanics without necessarily elucidating the underlying rationales, whereas explainability entails the capacity to elucidate the processes in a literal manner.\nThink of it this way: say you’re doing a science experiment at school. The experiment might be interpretable insofar as you can see what you’re doing, but it is only really explainable once you dig into the chemistry behind what you can see happening.\n","date":1687374840,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1687910400,"objectID":"e358a629f83ffa3a71a10ed3b2c7387b","permalink":"https://gmr523.github.io/post/explainability-vesus-interpretability/","publishdate":"2023-06-21T19:14:00Z","relpermalink":"/post/explainability-vesus-interpretability/","section":"post","summary":"In fact, the goal of dictionary learning is to extract the most essential features of objects, similar to words or terms found in a dictionary. Therefore, this process is referred to as dictionary learning.","tags":["Academic","开源"],"title":"Explainability versus Interpretability","type":"post"},{"authors":["Mingrong Gong"],"categories":["notes"],"content":" VScode remote SSH login without password # On the client side\ncat ~/.ssh/id_rsa.pub Then copy the token\n# On the server side\nvi ~/.ssh/authorized_keys ","date":1684696440,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684627200,"objectID":"fba8e3ffa71a7c0d10685c4ec70da402","permalink":"https://gmr523.github.io/post/useful-command/","publishdate":"2023-05-21T19:14:00Z","relpermalink":"/post/useful-command/","section":"post","summary":"In fact, the goal of dictionary learning is to extract the most essential features of objects, similar to words or terms found in a dictionary. Therefore, this process is referred to as dictionary learning.","tags":["Academic","开源"],"title":"Useful command","type":"post"},{"authors":["Mingrong Gong","Zhengkun Yi","Huiyun Li","Yunduan Cui","Xinyu Wu"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1682899200,"objectID":"a3a28693fa57ef0482f1ffc3c6e26a9b","permalink":"https://gmr523.github.io/publication/kdac/","publishdate":"2023-05-01T00:00:00Z","relpermalink":"/publication/kdac/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"Reduce Overestimation by Kullback-Leibler Divergence Regularized","type":"publication"},{"authors":["Yue Wang","Mingrong Gong","Lei Xia","Qieshi Zhang","Jun Cheng"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1661990400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1661990400,"objectID":"8b2eba57aabec48ef641f64ebb228028","permalink":"https://gmr523.github.io/publication/effisdepth/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/effisdepth/","section":"publication","summary":"Monocular self-supervised depth estimation with a low-cost sensor is the mainstream solution to gathering dense depth maps for robots and autonomous driving. In this paper, based on the philosophy “less is more” (i.e., focusing only on valid pixels in sparse LiDAR), we propose a novel framework, Efficient Sparse Depth (EffisDepth), for predicting dense depth. The Sparse Feature Extractor (SFE) embedded in the proposed framework effectively handles sparse LiDAR by forming sparse tensors. The Slender Group Block (SGB) is the main building block in SFE, which extracts features from sparse tensors via a structure of two branches. Extensive experiments show that our method achieves state-of-the-art performance on the KITTI benchmark, demonstrating the effectiveness of each proposed component and the self-supervised learning framework","tags":["Source Themes"],"title":"Efficiently Fusing Sparse LiDAR for Enhanced Self-Supervised Monocular Depth Estimation","type":"publication"},{"authors":["Mingrong Gong","Qieshi Zhang","Fusheng Hao","Shuiming Ouyang","Jun Cheng"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example. Supplementary notes can be added here, including code, math, and images.\n","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"1074c1bf7bd290eaf37e9e54d295ef90","permalink":"https://gmr523.github.io/publication/adnet/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/adnet/","section":"publication","summary":"Indoor scene parsing is a crucial and fundamental task in the field of robotics. In recent years, semantic segmentation based on RGB images and depth images have achieved excellent performance. However, the quality of depth images are not high, and often have noise or information loss. The spatial information that contained in the depth images is not in the same mode as the color information in the RGB images, which cannot be simply overlapped to segment. The limitation can lead to unsatisfactory segmentation results. To overcome this limitation, we propose Asymmetric Dual-mode Network (ADNet) to fuse color information and spatial information more efficiently. The core of ADNet is a shallow network for extracting spatial information in depth images, which can improve the utilization of the network. In addition, Depth Filter (DF) operator is added to the network to optimize the spatial information and reduce the effect of noise on segmentation. We evaluate our ADNet on the common indoor dataset NYUv2 and compare it with the approach of state-of-the-art on this dataset, our proposed model has a competitive performance.","tags":[],"title":"ADNet: Asymmetric Dual-mode Network for RGB-D Indoor Scene Parsing","type":"publication"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Wowchemy Wowchemy | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let’s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://gmr523.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Wowchemy's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://gmr523.github.io/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://gmr523.github.io/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"}]